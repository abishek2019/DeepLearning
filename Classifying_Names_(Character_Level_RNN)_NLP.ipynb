{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16cC81dWH-k-PJcovTA8VUgvVs5FUvJbo",
      "authorship_tag": "ABX9TyP3uANfxATmUJv+cTU2bVmX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishek2019/DeepLearning/blob/main/Classifying_Names_(Character_Level_RNN)_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess data (Get data_paths, read_data, transform and load)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as d\n",
        "import glob\n",
        "import unicodedata\n",
        "import string\n",
        "import os\n",
        "\n",
        "torch.manual_seed(1)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dir = '/content/drive/MyDrive/pytorch-Deep-Learning-master/nameclassifier_data/data/names/*.txt'\n",
        "# Hyperparamters\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "hidden_size = 128\n",
        "\n",
        "vocabulary = string.ascii_letters + '.,;'\n",
        "# The total size of all the allowed ascii characters that will train our language model.\n",
        "vocab_size = len(vocabulary)\n",
        "languages = []\n",
        "\n",
        "def unicodetoASCII(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in vocabulary)\n",
        "\n",
        "def lineToTensor(single_name):\n",
        "  single_name_tensor = torch.zeros(len(single_name), 1, vocab_size)\n",
        "  for i, letter in enumerate(single_name):\n",
        "    single_name_tensor[i][0][vocabulary.find(letter)] = 1\n",
        "  return single_name_tensor\n",
        "\n",
        "def labelToTensor(label):\n",
        "  return torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "# Custom dataset class\n",
        "class NamesDataset(d.Dataset):\n",
        "  def __init__(self, root_dir):\n",
        "    file_paths = glob.glob(root_dir)\n",
        "    self.names = []\n",
        "    self.labels = []\n",
        "    for i, each_file_path in enumerate(file_paths):\n",
        "      country_name = os.path.splitext(os.path.basename(each_file_path))[0]\n",
        "      languages.append(country_name)\n",
        "      names_list = open(each_file_path, encoding = 'utf-8').read().strip().split('\\n')\n",
        "      names_list = [unicodetoASCII(name) for name in names_list]\n",
        "      self.names.extend(names_list)\n",
        "      self.labels.extend([i] * len(names_list))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.names)\n",
        "\n",
        "  # not used\n",
        "  def __getitem__(self, index):\n",
        "    names = self.names[index]\n",
        "    labels = self.labels[index]\n",
        "    return names, labels\n",
        "\n",
        "dataset = NamesDataset(dir)\n",
        "# dataset.names = [all names], dataset.labels = [indices all labels]\n",
        "\n",
        "print(dataset.names)\n",
        "print(dataset.labels)\n",
        "print(languages)\n",
        "print(len(languages))\n",
        "\n",
        "# Not used in this program as we have generated random names for training. Used if training in batches.\n",
        "train_size = int(0.75 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = d.random_split(dataset, [train_size, test_size])\n",
        "trainloader = d.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = d.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "Y3YFvNywNt4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the model\n",
        "class RNNClassifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNNClassifier, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x, hidden_x):\n",
        "    combined = torch.cat((x, hidden_x), 1)\n",
        "    hidden_x = self.i2h(combined)\n",
        "    x = self.i2o(combined)\n",
        "    x = self.softmax(x)\n",
        "    return x, hidden_x\n",
        "\n",
        "  # Init the hidden states- helps RNN provide a consistent starting point and capture long-term dependencies(ability to retain earlier info).\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "output_size = len(languages)\n",
        "model = RNNClassifier(vocab_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "ccr4bvW8DoE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define loss and optimizer\n",
        "import torch.optim as optim\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PkXz4BAUTc7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model\n",
        "# Demo Train\n",
        "import random\n",
        "x = lineToTensor('Albert').to(device)\n",
        "hidden_x = torch.zeros(1, 128).to(device)\n",
        "output, next_hidden = model(x[0], hidden_x)\n",
        "print(output.shape)\n",
        "\n",
        "def randomLib():\n",
        "  names = dataset.names\n",
        "  random_name = names[random.randint(0, len(names) - 1)]\n",
        "  random_name_index = names.index(random_name)\n",
        "  name_label = dataset.labels[random_name_index]\n",
        "  name_tensor = lineToTensor(random_name).to(device)\n",
        "  label_tensor = labelToTensor(name_label).to(device)\n",
        "  return name_tensor, label_tensor\n",
        "\n",
        "def trainFromRandomName():\n",
        "  running_loss = 0.0\n",
        "  for i in range(10000):\n",
        "    # Get the data\n",
        "    name_tensor, label_tensor = randomLib()\n",
        "    hidden_x = model.init_hidden().to(device)\n",
        "    # Zero_grad\n",
        "    optimizer.zero_grad()\n",
        "    # Forward\n",
        "    for ix in range(name_tensor.size(0)):\n",
        "      output, hidden_x = model(name_tensor[ix], hidden_x)\n",
        "    loss = loss_fn(output, label_tensor)\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    # Optimize\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if ((i + 1) % 5000 == 0):\n",
        "      print(f'Epoch: {epoch + 1}  Iter: {i + 1}   Loss: {running_loss / 5000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "def trainFromTrainLoader():\n",
        "  print('-----TRAINING FROM TRAINLOADER-----')\n",
        "  running_loss = 0.0\n",
        "  for i, (names, labels) in enumerate(trainloader):\n",
        "    # Get the data\n",
        "    for ix, name in enumerate(names):\n",
        "      hidden_x = model.init_hidden().to(device)\n",
        "      name_tensor = lineToTensor(name).to(device)\n",
        "      label_tensor = labelToTensor(labels[ix]).to(device)\n",
        "      # Set Zero Grad\n",
        "      optimizer.zero_grad()\n",
        "      # Forward\n",
        "      for idx in range(name_tensor.size(0)):\n",
        "        output, hidden_x = model(name_tensor[idx], hidden_x)\n",
        "      loss = loss_fn(output, label_tensor)\n",
        "      # Backward\n",
        "      loss.backward()\n",
        "      # Optimize\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    if (i + 1) % 500 == 0:\n",
        "      print(f'Epoch: {epoch}     Batch: {i+1}    Loss = {running_loss/5000}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "model.train()\n",
        "print('-----TRAINING FROM RANDOM EXAMPLE-----')\n",
        "for epoch in range(epochs):\n",
        "  # trainFromTrainLoader()\n",
        "  trainFromRandomName()\n",
        "print('Training Finished')\n",
        "torch.save(model.state_dict(), 'modelRNN.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7YYQNhpUh7z",
        "outputId": "71da49e5-3018-43be-aa6d-a8e3eaf9f9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 18])\n",
            "-----TRAINING FROM RANDOM EXAMPLE-----\n",
            "Epoch: 1  Iter: 5000   Loss: 2.558\n",
            "Epoch: 1  Iter: 10000   Loss: 2.893\n",
            "Epoch: 2  Iter: 5000   Loss: 2.931\n",
            "Epoch: 2  Iter: 10000   Loss: 2.866\n",
            "Epoch: 3  Iter: 5000   Loss: 2.799\n",
            "Epoch: 3  Iter: 10000   Loss: 2.774\n",
            "Epoch: 4  Iter: 5000   Loss: 2.785\n",
            "Epoch: 4  Iter: 10000   Loss: 2.771\n",
            "Epoch: 5  Iter: 5000   Loss: 2.859\n",
            "Epoch: 5  Iter: 10000   Loss: 2.898\n",
            "Epoch: 6  Iter: 5000   Loss: 2.893\n",
            "Epoch: 6  Iter: 10000   Loss: 2.897\n",
            "Epoch: 7  Iter: 5000   Loss: 2.911\n",
            "Epoch: 7  Iter: 10000   Loss: 2.918\n",
            "Epoch: 8  Iter: 5000   Loss: 2.918\n",
            "Epoch: 8  Iter: 10000   Loss: 2.920\n",
            "Epoch: 9  Iter: 5000   Loss: 2.921\n",
            "Epoch: 9  Iter: 10000   Loss: 2.905\n",
            "Epoch: 10  Iter: 5000   Loss: 2.881\n",
            "Epoch: 10  Iter: 10000   Loss: 2.889\n",
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Test the model\n",
        "model = RNNClassifier(vocab_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(torch.load('modelRNN.pth'))\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "language_correct = {language: 0 for language in languages}\n",
        "each_language_total = {language: 0 for language in languages}\n",
        "# No gradient\n",
        "with torch.no_grad():\n",
        "  for i in range(20000):\n",
        "    # Get the data\n",
        "    name_tensor,label_tensor = randomLib()\n",
        "    hidden_x = model.init_hidden().to(device)\n",
        "    # Forward\n",
        "    for ix in range(name_tensor.size(0)):\n",
        "      output, hidden_x = model(name_tensor[ix], hidden_x)\n",
        "    # Max probabilty\n",
        "    _, prediction = torch.max(output.data, 1)\n",
        "    if prediction == label_tensor:\n",
        "      correct += 1\n",
        "      language_correct[languages[label_tensor.item()]] += 1\n",
        "    each_language_total[languages[label_tensor.item()]] += 1\n",
        "    total += 1\n",
        "print(f\"Total Acccuracy is {1000 * correct/total:.3f}%.\")\n",
        "# print('-----Classwise accuracy-----')\n",
        "# for language, values in language_correct.items():\n",
        "#   print(f'Language: {language}\\tTotal predicted: {each_language_total[language]}\\tAccuracy: {100 * values/ each_language_total[language]:.3f}%')"
      ],
      "metadata": {
        "id": "y7yAkmraXo_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f52b700-987f-414d-9f8c-e50ad3971f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Acccuracy is 79.900%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Predict\n",
        "input_name = 'Abi'\n",
        "with torch.no_grad():\n",
        "  hidden_x = model.init_hidden().to(device)\n",
        "  input_tensor = lineToTensor(input_name).to(device)\n",
        "  for letter_tensor in input_tensor:\n",
        "    output, hidden_x = model(letter_tensor, hidden_x)\n",
        "  # _, prediction = torch.max(output.data, 1)\n",
        "  # prediction = languages[prediction.item()]\n",
        "  number_of_predictions = 3\n",
        "  _, predictions = output.topk(number_of_predictions,1, True)\n",
        "  print(f'Input Name: {input_name}')\n",
        "  print(predictions)\n",
        "  for i in range(number_of_predictions):\n",
        "    value = _[0][i].item()\n",
        "    print(f'{languages[predictions[0][i].item()]} Probability: ({value:.3f})')\n",
        "\n"
      ],
      "metadata": {
        "id": "AWUFECaOXpJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a25a7d-d9e9-46a8-9d30-73f164f4f08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Name: Abi\n",
            "tensor([[5, 3, 4]])\n",
            "Chinese Probability: (1.000)\n",
            "Arabic Probability: (0.000)\n",
            "English Probability: (0.000)\n"
          ]
        }
      ]
    }
  ]
}